## rpc与mapreduce

### 项目简介

本项目基于netty实现通用的rpc框架，rpc分别实现了服务端与客户端，服务由接口+类的形式提供，服务端需要实现服务接口并将实现类注册到本地注册中心，此外，服务端还需要将提供的服务名称（类.方法）注册到远程注册中心zookeeper，客户端通过服务端的注册中心拿到提供服务的所有server信息，通过netty进行通信拿到服务端的执行结果，客户端提供代理模式供选择，代理接口直接提供了访问远程客户端的方法封装，直接调用就可获得服务对象，直接调用本地方法就可以实现rpc远程调用。在实现rpc的基础上，逐渐开启mit6.824的学习。该项目在实现上比较简化，只追求跑通理解概念，大体上比较容易理解，许多细节还有思考的空间。

### 使用方式

客户端和服务端应该知道他们的通信方式和内容，这里让客户端和服务端都拥有一个公共的接口方便客户端可以直接调用，本质上这里可以使用不同的数据交换协议，例如protobuf等

服务端

```
 //开启一个server，指定ip端口提供服务（这里使用ServerPool管理server，也可以直接创建）
 RpcServer rpcServer = ServerPool.get("127.0.0.1:8083");
 //注册服务，第一个参数是服务端提供的对应接口的具体实现类，第二个参数是提供服务的对象实例，这里new是让服务过程中不重复new对象，第三个参数代表提供服务的具体方法
 rpcServer.register(Echo.class, new Echo(), "echo");
 //启动服务
 rpcServer.start();
```

客户端

```
//使用BeanProxy直接代理实现对应接口，代理方法请求了远程server服务
Message echo = (Message)BeanProxy.getBean(Message.class, "Echo");
//直接调用就可获取服务端执行结果
String res = echo.echo("huachuan");
System.out.println(res);
```



### 项目架构

#### rpc架构

<img src=".\images\rpc.png" alt="rpc" />

上图可以描述为如下几个步骤

- server端向本地注册中心添加服务，添加服务后server会将该服务同步到zookeeper注册中心，zookeeper中的组织结构方式如上图所示。

- 客户端在初始化时，会启动一个本地缓存去获取zookeeper的注册信息，避免每次请求都会请求zookeeper，通过注册watcher，当zookeeper注册新的服务或者发生改变时客户端能够及时感知并更新本地缓存。

- 客户端请求服务时，首先去获取缓存中的服务注册信息，若获取失败，则说明没有提供该服务的server，获取成功后可以自定义负载均衡策略获取server，本实现只简单地随机获取server进行处理。

- 客户端可以使用实现的BeanProxy工具类获取服务的代理实现，代理实现封装了客户端调用服务的一系列过程。

  

#### mapreduce框架

mapreduce主要通过map和reduce两个操作来完成一个复杂的任务，其主要通过master和多个worker组成，master负责分发任务和错误处理，worker负责具体任务的完成（map or reduce)，本实现的map和reduce的任务是完成文件中的单词统计。

<img src=".\images\mapReduce.png" alt="rpc" style="zoom:50%;" />

在master启动阶段，需要告知master需要处理的文件夹路径（默认在resources/wordCountFiles/），在启动后，master需要不断监听以此来连接worker，当有worker连接后便可以给其分发任务，在worker工作的整个过程中，master会不断地发送心跳检测以此来判断当前worker的健康状况，如果当前worker不能正常响应，master就会主动断开连接(当然，这里可以有很多的判定策略)。worker在初始化时，会开启能够供master调用的本地服务，当worker初始化完成后，就会主动与master发起连接请求，连接成功后就可以正常提供服务了。

在跨机器的处理时，非常容易因为网络故障或者机器宕机而造成分发的任务不能按时完成，从而造成系统的可靠性降低，所以容错处理是非常必要的，master作为整个系统的协调者，应该需要考虑很多情况：

1.worker断开连接或者无响应怎么办？

master只要未得到worker的响应结果，会等待至少一个心跳周期，若在该周期内worker主动断开连接或者周期心跳无反应，master感知后会主动断开连接并将该任务重新放回任务队列进行再次分配。

2.如何保证任务的有序发放，做到不丢失，不重复？

在任务的发放过程中分层次发放，先分发map任务，在分发reduce任务，这样可以减少并发带来的效率问题。由于master的任务确认机制，不丢失是能够做到的，但是不重复需要通过额外的机制进行实现，比如map完成后进行去重等，本项目中的任务为统计单词个数，可以通过文件系统自身的机制实现不重复。

3.多线程并发问题？

mapreduce中的线程并发问题主要集中在对worker资源以及任务资源的并发访问上，在分配worker和任务时，均使用了锁来实现有序并发（对于任务多的场景可以使用CAS和预分配策略）。



#### raft框架

在raft模块中实现了多个节点的leader选举和日志同步复制功能，一个raft节点可以与其他节点相互通信，一个节点既是一个服务，也是客户端与其他节点通信共同完成如上两个功能。

##### leader选举

在leader选举功能实现需要考虑的主要问题有两个：

- 如何确保最终能选出一个leader节点
- 如何确保不能发生脑裂现象（即在同一时间内不能同时出现多个leader）

对于第一点来讲，论文中通过任期号作为逻辑时序，每个节点处在自旋等待状态，在一定时间过后没有收到leader的心跳信息便会认为当前集群中没有leader节点，此时，当前节点的任期号加一并开始发起选举，请求其他节点为其投票，当该节点能够获取大多数节点的投票，该节点就能成为leader节点，在这个过程中，如果当前任期内没有能够选出leader，即所有节点都未获取大多数节点的投票，那么在随机时间后，节点又会重复以上投票过程，直到产生leader为止。

对于第二点，要保证整个集群中只产生一个leader，因为只有获取到大多数选票的节点才能成为leader，且在一个任期内每个节点只有一张选票，要么投给自己，要么投给别人，所有可以确保在一个任期内只产生一个leader，在一个时间点，可能许多节点的任期并不相同，这就意味着许多节点在跨任期投票，这应该是不被允许的，所以当节点收到高任期的投票，自己需要放弃竞选，并将自己的任期改为高任期并投出一票，这样就可以制止低任期的无效投票以及产生多个不同任期的leader。

为了便于日志同步的一致性，在leader选举时，比较新的节点会拒绝比较旧的节点的投票请求（Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新）

##### 日志同步

日志同步需要考虑如下问题：

- 如何确定一条日志被写入成功（当前日志可以被提交）
- 不同节点间的日志如何保证一致
- 如何保证发生异常的情况下保持日志的一致性
  - leader断开重连
  - follower断开重连

当我们在对集群写入一条日志时，首先，如果集群的状态未就绪，leader未选举成功/成功运行节点个数不到一半，此时是不能向集群写入日志的，如果leader选举成功，对于日志的写入全部由leader负责，如果请求发送到follower，需要follower转发请求到leader，leader写入后负责向follower节点进行日志同步，当有超过一半节点都写入当前日志时，就可以确保当前日志被写入成功，因为当大多数都写入后，还未写入的节点肯定不能当选成leader，未写入的节点最后一条日志不如写入后的新，且未写入的节点不是大多数，少数的投票是不能成为leader的，所以可以认为大多数节点写入的日志就是成功的，即以后不会被覆盖。

在进行日志同步时，leader节点的日志就是真理，leader会同步它自身的节点到其他节点，而不会改变自己日志，leader日志的同步包含在心跳请求中，leader会首先确定好与follower最后一条一致的日志，由于raft日志写入的原则可以保证在这条日志之前的所有日志都是一致的（日志匹配特性），一条日志由索引号和任期号以及日志内容组成，如果两个节点的日志在某一条出相同，那么说明之前的日志复制过程中两个节点是完全一致的（类似演绎归纳）。在找到相同日志后，就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。

> 注：对于日志匹配特性的一点理解
>
> 对于一个任期，leader是唯一的，产生的日志也是唯一的，也就意味着当不同节点某条日志相同时，假设之前某一条日志不同，index肯定相同，那就意味着该索引位置日志产生的任期不同，而这条日志后面的任期相同，说明是某个leader任期内产生的日志被复制到不同的日志后面，但是在复制时的leader前面一位只能是一个固定的任期，所以不可能产生这种情况。

### 技术要点

- netty通信
- zookeeper注册中心
- 动态代理 
- 多线程同步
- maven分模块
- raft算法
- git操作（设置.gitignore文件）

